{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font face=\"times\">\n",
    "  \n",
    "# 1. Policy Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font face=\"times\">\n",
    " \n",
    "## 1.1. Notebook overview \n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\">This Notebook is based on the pre-trained sentiment model SKEP (Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis) , which is fine-tuned to conduct sentiment analysis on Wuhan's anti-epidemic transport policy during the COVID-19 outbreak.A balance needs to be struck between the risk of virus transmission and supporting essential travel activities during a pandemic. In which transportation can be considered as a vector from an epidemiological perspective, however, modern cities cannot be sustained without the continued delivery and operation of systems such as food, fuel, power, and medical care. Also, in the context of lack of extensive historical knowledge, dynamic tracking and adjustment of emergency policy become imperative.Thus, sentiment analysis measures the satisfactory level of the public by classifying people's response to policy into positive, negative, and neutral categories, with positive public sentiment meaning a higher level of policy acceptance while negative public sentiment demonstrates a lower policy acceptance.</p></font>\n",
    "\n",
    "## 1.2. Pre-trained sentiment analysis model SKEP\n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\">In recent years, a large body of research has shown that pre-trained models (PTMs) based on large corpora can learn generic language representations that are beneficial for downstream NLP tasks, while avoiding the need to train models from scratch. With the development of computational power, the emergence of deep models (i.e., Transformer) and the enhancement of training skills have allowed PTMs to evolve from shallow to deep.</p></font>\n",
    "    \n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\"> The Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis (SKEP), a pre-training model for sentiment analysis, outperforms SOTA on 14 typical tasks for Chinese-English sentiment analysis, and this work has been accepted by ACL 2020. SKEP is a sentiment knowledge augmentation-based sentiment pre-training algorithm proposed by the Baidu research team, which uses unsupervised methods to automatically mine sentiment knowledge and then use the sentiment knowledge to construct pre-training targets so that machines can learn to understand sentiment semantics.</p></font>\n",
    "\n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\"> The Baidu research team further validated the effectiveness of the sentiment pre-training model SKEP on three typical sentiment analysis tasks, Sentence-level Sentiment Classification, Aspect-level Sentiment Classification, Opinion Role Labeling), a total of 14 Chinese and English data to further validate the effect of the sentiment pre-training model SKEP. For specific experimental results, please refer to: https://github.com/baidu/Senta#skep. The paper is available at: https://arxiv.org/abs/2005.05635.</p></font>\n",
    "\n",
    "## 1.3. Dataset\n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\">At first, a random sample (for each label, 6 times, 1000 each) is drawn from the data set labeled directly with the pre-trained model wrapper library Senta.This yielded a sample of 18,000 data with label \"0\" for negative, label \"1\" for neutral, and label \"2\" for positive, with text lengths between 10 and 153.</p></font>\n",
    "  \n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\">Next, manual checking is performed.For policy responses, objective and rational views with the subject matter of facts, announcements, inquiries/consultations, persuasions, and suggestions were considered as sentimentally neutral; further, views with complaints, rebuttals, accountability, speculations/questions, and orders were considered as sentimentally negative; and finally, views with hope, compliments, approval, support, prayers, and blessings/wishes were considered as sentimentally positive.</p></font>\n",
    "  \n",
    "<font size=4><p align = \"justify\" style=\"line-height:180%\">Finally, the average precision on 18,000 items is 69.3%, with the label \"2\" represents positive precision up to 82.4%, negative and neutral are 69.1% and 56.4%, respectively; the average recall is 60.6%, with the label \"0\" represents negative recall up to 81.1%, positive and neutral are 32.2% and 68.5%, respectively; further, the manually labeled dataset is used as the fine-tuned dataset of the pre-trained SKEP model, in which the amount of negative data is 5869, neutral data is 5893, and positive data was 3238.</p></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font face=\"times\">\n",
    "  \n",
    "# 2. Precision and Recall analysis under the condition of directly using the sentiment pre-trained model SKEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data0.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "data0 = pd.read_csv('data0.csv')\n",
    "data1 = pd.read_csv('data1.csv')\n",
    "data2 = pd.read_csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-02 02:02:20.934560\n",
      "Recall for each data_set with single label: 0.8068954085861098\n",
      "2022-08-02 02:02:20.982996\n",
      "Recall for each data_set with single label: 0.6847960444993819\n",
      "2022-08-02 02:02:21.159078\n",
      "Recall for each data_set with single label: 0.5319148936170213\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "import datetime\n",
    "\n",
    "def pre_trained_error_each(data):\n",
    "    nan_index = data[np.isnan(data['label1'])].index\n",
    "    data_ = copy.deepcopy(data)\n",
    "    data_ = data_.drop(nan_index)\n",
    "    data_.index = np.arange(len(data_))\n",
    "    #===\n",
    "    int_label1 = []\n",
    "    for i in range(len(data_)):\n",
    "        if data_['label1'][i]==9.0:\n",
    "             int_label1.append(int(0))\n",
    "        elif data_['label1'][i]==11.0:\n",
    "            int_label1.append(int(1))\n",
    "        elif data_['label1'][i]==22.0:\n",
    "            int_label1.append(int(2))\n",
    "        elif data_['label1'][i]==00.0:\n",
    "            int_label1.append(int(0))\n",
    "        else:\n",
    "            int_label1.append(int(data_['label1'][i]))\n",
    "    #==\n",
    "    data_['label1'] = int_label1\n",
    "    #==\n",
    "    label = list(data_['label'])\n",
    "    label1 = list(data_['label1'])\n",
    "    label_ = dict(Counter(label))#a\n",
    "    label1_ = dict(Counter(label1))#b\n",
    "    #==\n",
    "    time1 = datetime.datetime.now()\n",
    "    print(time1)\n",
    "    print('Recall for each data_set with single label:',label1_[list(label_.keys())[0]]/list(label_.values())[0])\n",
    "\n",
    "pre_trained_error_each(data0)\n",
    "pre_trained_error_each(data1)\n",
    "pre_trained_error_each(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-02 02:09:08.299299\n",
      "precision_0 0.7892501819946615\n",
      "precision_1 0.35346358792184723\n",
      "precision_2 0.9018895348837209\n",
      "2022-08-02 02:09:08.721291\n",
      "recall_0 0.8097846383667372\n",
      "recall_1 0.6743476787529651\n",
      "recall_2 0.31143101482326113\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def average_pre_trained_error(data0,data1,data2):\n",
    "    def data_dframe(data):\n",
    "        nan_index = data[np.isnan(data['label1'])].index\n",
    "        data_ = copy.deepcopy(data)\n",
    "        data_ = data_.drop(nan_index)\n",
    "        data_.index = np.arange(len(data_))\n",
    "        #===\n",
    "        int_label1 = []\n",
    "        for i in range(len(data_)):\n",
    "            if data_['label1'][i]==9.0:\n",
    "                int_label1.append(int(0))\n",
    "            elif data_['label1'][i]==11.0:\n",
    "                int_label1.append(int(1))\n",
    "            elif data_['label1'][i]==22.0:\n",
    "                int_label1.append(int(2))\n",
    "            elif data_['label1'][i]==00.0:\n",
    "                int_label1.append(int(0))\n",
    "            else:\n",
    "                int_label1.append(int(data_['label1'][i]))\n",
    "        #==\n",
    "        data_['label1'] = int_label1\n",
    "        return data_\n",
    "    #==\n",
    "    data0_ = data_dframe(data0)\n",
    "    data1_ = data_dframe(data1)\n",
    "    data2_ = data_dframe(data2)\n",
    "    #===\n",
    "    data_all = pd.concat([data0_,data1_,data2_,data0_.iloc[:2000,:],data1_.iloc[:1333,:],data2_.iloc[:1000,:]])\n",
    "    data_all.index = np.arange(len(data_all))\n",
    "    #==data_all\n",
    "    #==Precision\n",
    "    def get_precision(data_all,label_index):\n",
    "        if label_index == 0:\n",
    "             true_label0_index = [i for i in range(len(data_all)) if data_all['label1'][i]==0]\n",
    "             recognize_label = [data_all['label'][i] for i in true_label0_index] \n",
    "             recognize_label_ = dict(Counter(recognize_label))\n",
    "             precision_0 = recognize_label_[0]/len(true_label0_index)\n",
    "             print('precision_0',precision_0)\n",
    "        elif label_index == 1:\n",
    "            true_label1_index = [i for i in range(len(data_all)) if data_all['label1'][i]==1]\n",
    "            recognize_label = [data_all['label'][i] for i in true_label1_index] \n",
    "            recognize_label_ = dict(Counter(recognize_label))\n",
    "            precision_1 = recognize_label_[1]/len(true_label1_index)\n",
    "            print('precision_1',precision_1)\n",
    "        elif label_index == 2:\n",
    "            true_label2_index = [i for i in range(len(data_all)) if data_all['label1'][i]==2]\n",
    "            recognize_label = [data_all['label'][i] for i in true_label2_index] \n",
    "            recognize_label_ = dict(Counter(recognize_label))\n",
    "            precision_2 = recognize_label_[2]/len(true_label2_index)\n",
    "            print('precision_2',precision_2)\n",
    "    #==\n",
    "    time1 = datetime.datetime.now()\n",
    "    print(time1)\n",
    "    get_precision(data_all,0)\n",
    "    get_precision(data_all,1)\n",
    "    get_precision(data_all,2)\n",
    "    #==Recall\n",
    "    def get_recall(data_all,label_index):\n",
    "        if label_index == 0:\n",
    "            recognize_label0_index = [i for i in range(len(data_all)) if data_all['label'][i]==0]\n",
    "            true_label = [data_all['label1'][i] for i in recognize_label0_index] \n",
    "            true_label_ = dict(Counter(true_label))\n",
    "            recall_0 = true_label_[0]/len(recognize_label0_index)\n",
    "            print('recall_0',recall_0)\n",
    "        elif label_index == 1:\n",
    "            recognize_label1_index = [i for i in range(len(data_all)) if data_all['label'][i]==1]\n",
    "            true_label = [data_all['label1'][i] for i in recognize_label1_index] \n",
    "            true_label_ = dict(Counter(true_label))\n",
    "            recall_1 = true_label_[1]/len(recognize_label1_index)\n",
    "            print('recall_1',recall_1)\n",
    "        elif label_index == 2:\n",
    "            recognize_label2_index = [i for i in range(len(data_all)) if data_all['label'][i]==2]\n",
    "            true_label = [data_all['label1'][i] for i in recognize_label2_index] \n",
    "            true_label_ = dict(Counter(true_label))\n",
    "            recall_2 = true_label_[1]/len(recognize_label2_index)\n",
    "            print('recall_2',recall_2)\n",
    "    #==\n",
    "    time1 = datetime.datetime.now()\n",
    "    print(time1)\n",
    "    get_recall(data_all,0)\n",
    "    get_recall(data_all,1)\n",
    "    get_recall(data_all,2)\n",
    "    #==\n",
    "    return data_all\n",
    "\n",
    "data_all = average_pre_trained_error(data0,data1,data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_all = data_all.loc[:,['pure_blog','label1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#===增加一列\n",
    "data_all.insert(0,'tweet_id',np.array([i for i in range(len(data_all))]))\n",
    "data_all.to_csv('data_all.csv',sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id,pure_blog,label1\r\n",
      "0,都要生了管他禁不禁行，直接开到医院就行了,0\r\n",
      "1,封城只是不能出，从外地空运货运都行啊,1\r\n",
      "2,怎么说呢，什么事都没有十全十美，只能以大局为重了,1\r\n",
      "3,昨天报了12345，这个进度有点慢,0\r\n",
      "4,这是个血的教训！责任是谁的，谁也别想跑！！！赎罪吧！,0\r\n",
      "5,你到了吗？,0\r\n",
      "6,而且他们不够重视，医疗水平更是有限,0\r\n",
      "7,不要这么形容好不好？有人会伤心的,1\r\n",
      "8,作为京东员工我想说京东物流也不放假，而且京东商城抵制商家涨价，,0\r\n"
     ]
    }
   ],
   "source": [
    "!head data_all.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font face=\"times\">\n",
    "  \n",
    "# 3. Fine Tuning pretrained SKEP model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'pure_blog', 'label1'], dtype='object')\n",
      "Counter({0: 8242, 1: 5630, 2: 4128})\n",
      "tweet_id,pure_blog,label1\r\n",
      "0,都要生了管他禁不禁行，直接开到医院就行了,0\r\n",
      "1,封城只是不能出，从外地空运货运都行啊,1\r\n",
      "2,怎么说呢，什么事都没有十全十美，只能以大局为重了,1\r\n",
      "3,昨天报了12345，这个进度有点慢,0\r\n",
      "4,这是个血的教训！责任是谁的，谁也别想跑！！！赎罪吧！,0\r\n",
      "5,你到了吗？,0\r\n",
      "6,而且他们不够重视，医疗水平更是有限,0\r\n",
      "7,不要这么形容好不好？有人会伤心的,1\r\n",
      "8,作为京东员工我想说京东物流也不放假，而且京东商城抵制商家涨价，,0\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "data_all = pd.read_csv('data_all.csv')\n",
    "print(data_all.columns)\n",
    "print(Counter(data_all['label1']))\n",
    "!head data_all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#==========Data set splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_idx, val_test_idx, _, y_validate_test = train_test_split(data_all.index, data_all['label1'], stratify=data_all['label1'], \n",
    "train_size=0.9,test_size=1-0.9,random_state=2, shuffle=True)\n",
    "#print(list(val_test_idx))\n",
    "#=====\n",
    "data_all_train = data_all.drop(list(val_test_idx))\n",
    "#data_all_train.index = np.arange(data_all_train)\n",
    "data_all_train.to_csv('data_all_train.csv',index=False)\n",
    "\n",
    "#===\n",
    "data_all_test = data_all.drop(list(train_idx))\n",
    "#data_all_test.index = np.arange(data_all_test)\n",
    "data_all_test.to_csv('data_all_test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id,pure_blog,label1\n",
      "25,既然国家封城自然会做好相应保障工作。,1\n",
      "26,你想多了吧这节骨眼要把儿子接出来，儿子坐高铁坐飞机就得了干嘛她跑进来再跑出去？,0\n",
      "34,全部采用无人机配送物资，人员。无人机分类为，送粮食的，器材的，送人的，统一调度，实时监控。这样就大幅度隔离了人员往来。疫情会马上下降甚至终止。问题是，这样的无人机可有？尤其是送医护人员的。,0\n",
      "35,他有瞒报，死了几百个了确诊几千个，记者不敢报因为不让报,0\n",
      "40,我只想问孕妇怎么办，马上就要生了，半夜发作去找谁,0\n",
      "44,问题是他到了新地方再查出来不也很糟糕吗,0\n",
      "55,口罩到底有用吗南阳地区离武汉好近！,1\n",
      "57,洗手口罩卫生有病及时就医。。。。。。,1\n",
      "75,你们的医护都是尽可能多的派过来了，可是一线管理人员不够能支援吗，除了医护，其他人敢过来帮我们管理社区吗？其实封城的意思是所有人都不能出去了，包括买菜买药和看病的，一律禁足，一个人都不能出去，像其他省那样封小区，每几天派个人出去一趟，我们早就是这样了，封城那天开始就相当于封区了,0\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "!head data_all_test.csv\n",
    "print(max([len(i) for i in data_all['pure_blog']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read(pd_data):\n",
    "    for index, item in pd_data.iterrows():       \n",
    "        yield {'text': item['pure_blog'], 'label': item['label1'], 'qid': item['tweet_id']}\n",
    "\n",
    "data_all_train = pd.read_csv('data_all_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 分割训练集、测试机\n",
    "from paddle.io import Dataset, Subset\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle\n",
    "\n",
    "dataset = load_dataset(read, pd_data=data_all_train,lazy=False)\n",
    "\n",
    "dev_ds = Subset(dataset=dataset, indices=[i for i in range(len(dataset)) if i % 20== 1])\n",
    "train_ds = Subset(dataset=dataset, indices=[i for i in range(len(dataset)) if i % 20 != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "paddle.get_device()\n",
    "# 设置gpu训练\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\n",
    "if use_gpu:\n",
    "    paddle.set_device('gpu:0')\n",
    "    print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '都要生了管他禁不禁行，直接开到医院就行了', 'label': 0, 'qid': 0}\n",
      "{'text': '怎么说呢，什么事都没有十全十美，只能以大局为重了', 'label': 1, 'qid': 2}\n",
      "{'text': '昨天报了12345，这个进度有点慢', 'label': 0, 'qid': 3}\n",
      "{'text': '这是个血的教训！责任是谁的，谁也别想跑！！！赎罪吧！', 'label': 0, 'qid': 4}\n",
      "{'text': '你到了吗？', 'label': 0, 'qid': 5}\n",
      "2022-08-02 18:44:42.611224\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])\n",
    "import datetime\n",
    "time_now = datetime.datetime.now()\n",
    "print(time_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15390\n",
      "810\n"
     ]
    }
   ],
   "source": [
    "# 在转换为MapDataset类型\n",
    "train_ds = MapDataset(train_ds)\n",
    "dev_ds = MapDataset(dev_ds)\n",
    "print(len(train_ds))\n",
    "print(len(dev_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-02 18:44:46,976] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams and saved to /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch\n",
      "[2022-08-02 18:44:46,979] [    INFO] - Downloading skep_ernie_1.0_large_ch.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams\n",
      "100%|██████████| 1238309/1238309 [00:23<00:00, 52608.12it/s]\n",
      "W0802 18:45:10.663950    98 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0802 18:45:10.668140    98 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "[2022-08-02 18:45:18,480] [    INFO] - Downloading skep_ernie_1.0_large_ch.vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.vocab.txt\n",
      "100%|██████████| 55/55 [00:00<00:00, 2981.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# 指定模型名称一键加载模型\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "model = SkepForSequenceClassification.from_pretrained(\n",
    "    'skep_ernie_1.0_large_ch', num_classes=  3)\n",
    "# 指定模型名称一键加载tokenizer\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from visualdl import LogWriter\n",
    "\n",
    "writer = LogWriter(\"./log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "   \n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label：情感极性类别\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid：每条数据的编号\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "    \n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == \"train\":\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset, batch_sampler=sampler, collate_fn=batchify_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    # print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return  np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "\n",
    "# 文本序列最大长度166\n",
    "max_seq_length = 160\n",
    "# 批量数据大小\n",
    "batch_size = 64\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 50\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import time\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    lazy_mode = True,\n",
    "    grad_clip=clip,\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "#==\n",
    "clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)\n",
    "optimizer1 = paddle.optimizer.SGD(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    grad_clip=clip)\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 12050, epoch: 50, batch: 241, loss: 0.00001, accu: 0.99968, speed: 0.56 step/s\r"
     ]
    }
   ],
   "source": [
    "# 开启训练\n",
    "global_step = 0\n",
    "best_val_acc=0\n",
    "tic_train = time.time()\n",
    "best_accu = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        # 喂数据给model\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # 计算损失函数值\n",
    "        loss = criterion(logits, labels)\n",
    "        # 预测分类概率值\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        # 计算acc\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 10 == 0:\n",
    "            time_now = datetime.datetime.now()\n",
    "            print(time_now)\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "\n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 100 == 0 :\n",
    "            # 评估当前训练的模型\n",
    "            eval_loss, eval_accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "            print(\"eval  on dev  loss: {:.8}, accu: {:.8}\".format(eval_loss, eval_accu))\n",
    "            # 加入eval日志显示\n",
    "            writer.add_scalar(tag=\"eval/loss\", step=global_step, value=eval_loss)\n",
    "            writer.add_scalar(tag=\"eval/acc\", step=global_step, value=eval_accu)\n",
    "            # 加入train日志显示\n",
    "            writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\n",
    "            writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\n",
    "            save_dir = \"best_checkpoint\"\n",
    "            # 加入保存       \n",
    "            if eval_accu>best_val_acc:\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.mkdir(save_dir)\n",
    "                best_val_acc=eval_accu\n",
    "                print(f\"模型保存在 {global_step} 步， 最佳eval准确度为{best_val_acc:.8f}！\")\n",
    "                save_param_path = os.path.join(save_dir, 'best_model.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                fh = open('best_checkpoint/best_model.txt', 'w', encoding='utf-8')\n",
    "                fh.write(f\"模型保存在 {global_step} 步， 最佳eval准确度为{best_val_acc:.8f}！\")\n",
    "                fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font face=\"times\">\n",
    "  \n",
    "# 4. Test the Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#========================================================================test\n",
    "#========================================================================test\n",
    "# 数据读取\n",
    "import pandas as pd\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddle.io import Dataset, Subset\n",
    "from paddlenlp.datasets import MapDataset\n",
    "\n",
    "\n",
    "test = pd.read_csv('data_all_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'pure_blog', 'label1'], dtype='object')\n",
      "   tweet_id                                          pure_blog  label1\n",
      "0        25                                 既然国家封城自然会做好相应保障工作。       1\n",
      "1        26            你想多了吧这节骨眼要把儿子接出来，儿子坐高铁坐飞机就得了干嘛她跑进来再跑出去？       0\n",
      "2        34  全部采用无人机配送物资，人员。无人机分类为，送粮食的，器材的，送人的，统一调度，实时监控。这...       0\n",
      "3        35                        他有瞒报，死了几百个了确诊几千个，记者不敢报因为不让报       0\n",
      "4        40                           我只想问孕妇怎么办，马上就要生了，半夜发作去找谁       0\n"
     ]
    }
   ],
   "source": [
    "print(test.columns)\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "def read_test(pd_data):\n",
    "    for index, item in pd_data.iterrows():       \n",
    "        yield {'text': item['pure_blog'], 'label': item['label1'], 'qid': item['tweet_id']}\n",
    "\n",
    "test_ds =  load_dataset(read_test, pd_data=test,lazy=False)\n",
    "# 在转换为MapDataset类型\n",
    "test_ds = MapDataset(test_ds)\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "   \n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label：情感极性类别\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid：每条数据的编号\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "    \n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == \"train\":\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset, batch_sampler=sampler, collate_fn=batchify_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-03 14:28:12,079] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams and saved to /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch\n",
      "[2022-08-03 14:28:12,083] [    INFO] - Downloading skep_ernie_1.0_large_ch.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams\n",
      "100%|██████████| 1238309/1238309 [00:38<00:00, 31768.72it/s]\n",
      "W0803 14:28:51.184937   145 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0803 14:28:51.189386   145 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "[2022-08-03 14:28:59,631] [    INFO] - Downloading skep_ernie_1.0_large_ch.vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.vocab.txt\n",
      "100%|██████████| 55/55 [00:00<00:00, 3404.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 指定模型名称一键加载模型\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "model = SkepForSequenceClassification.from_pretrained(\n",
    "    'skep_ernie_1.0_large_ch', num_classes=  3)\n",
    "# 指定模型名称一键加载tokenizer\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "batch_size=16\n",
    "max_seq_length=256\n",
    "# 处理测试集数据\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from best_checkpoint/best_model.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "import os\n",
    "\n",
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'best_checkpoint/best_model.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "model.eval()\n",
    "for batch in test_data_loader:\n",
    "    input_ids, token_type_ids, qids = batch\n",
    "    # 喂数据给模型\n",
    "    logits = model(input_ids, token_type_ids)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    qids = qids.numpy().tolist()\n",
    "    results.extend(zip(qids, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4], 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_0: 0.9138349514563107\n",
      "precision_1: 0.8312611012433393\n",
      "precision_2: 0.8861985472154964\n",
      "average_precision: 0.8770981999717155\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "true_label = list(test['label1'])\n",
    "predict_label = [i[1] for i in results]\n",
    "\n",
    "#==\n",
    "def get_precision(true_label,predict_label,label_index):\n",
    "        if label_index == 0:\n",
    "             true_label0_index = [i for i in range(len(true_label)) if true_label[i]==0]\n",
    "             recognize_label = [predict_label[i] for i in true_label0_index] \n",
    "             recognize_label_ = dict(Counter(recognize_label))\n",
    "             precision_0 = recognize_label_[0]/len(true_label0_index)\n",
    "             print('precision_0:',precision_0)\n",
    "             return precision_0\n",
    "        elif label_index == 1:\n",
    "            true_label1_index = [i for i in range(len(true_label)) if true_label[i]==1]\n",
    "            recognize_label = [predict_label[i] for i in true_label1_index] \n",
    "            recognize_label_ = dict(Counter(recognize_label))\n",
    "            precision_1 = recognize_label_[1]/len(true_label1_index)\n",
    "            print('precision_1:',precision_1)\n",
    "            return precision_1\n",
    "        elif label_index == 2:\n",
    "            true_label2_index = [i for i in range(len(true_label)) if true_label[i]==2]\n",
    "            recognize_label = [predict_label[i] for i in true_label2_index] \n",
    "            recognize_label_ = dict(Counter(recognize_label))\n",
    "            precision_2 = recognize_label_[2]/len(true_label2_index)\n",
    "            print('precision_2:',precision_2)\n",
    "            return precision_2\n",
    "#===\n",
    "precision_0 = get_precision(true_label,predict_label,0)\n",
    "precision_1 = get_precision(true_label,predict_label,1)\n",
    "precision_2 = get_precision(true_label,predict_label,2)\n",
    "average_precision = (precision_0+precision_1+precision_2)/3\n",
    "print('average_precision:',average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font face=\"times\">\n",
    "  \n",
    "# 5. Prediction of other remaining data using fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-03 16:03:13,529] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2022-08-03 16:03:17,410] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from best_checkpoint/best_model.pdparams\n"
     ]
    }
   ],
   "source": [
    "#=================================================================================\n",
    "# 数据读取\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddle.io import Dataset, Subset\n",
    "from paddlenlp.datasets import MapDataset\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "   \n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label：情感极性类别\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid：每条数据的编号\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, qid\n",
    "#======================================================================\n",
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "    \n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == \"train\":\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset, batch_sampler=sampler, collate_fn=batchify_fn)\n",
    "    return dataloader\n",
    "#======================================================================\n",
    "# 指定模型名称一键加载模型\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "model = SkepForSequenceClassification.from_pretrained(\n",
    "    'skep_ernie_1.0_large_ch', num_classes=  3)\n",
    "# 指定模型名称一键加载tokenizer\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\n",
    "\n",
    "#======================================================================\n",
    "# 加载模型\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'best_checkpoint/best_model.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "#======================================================================\n",
    "\n",
    "def get_label_tocsv(policy_name):\n",
    "    import numpy as np\n",
    "    print(policy_name)\n",
    "    test = pd.read_csv('comment_{}.csv'.format(policy_name))\n",
    "    #===增加一列\n",
    "    test.insert(0,'tweet_id',np.array([i for i in range(len(test))]))\n",
    "    #===\n",
    "    def read_test(pd_data):\n",
    "        for index, item in pd_data.iterrows():       \n",
    "            yield {'text': item['pure_blog'], 'qid': item['tweet_id']}\n",
    "\n",
    "    test_ds =  load_dataset(read_test, pd_data=test,lazy=False)\n",
    "    # 在转换为MapDataset类型\n",
    "    test_ds = MapDataset(test_ds)\n",
    "    #======================================================================\n",
    "    from functools import partial\n",
    "    import numpy as np\n",
    "    import paddle\n",
    "    import paddle.nn.functional as F\n",
    "    from paddlenlp.data import Stack, Tuple, Pad\n",
    "    batch_size=16\n",
    "    max_seq_length=256\n",
    "    # 处理测试集数据\n",
    "    trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "    ): [data for data in fn(samples)]\n",
    "    test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "    #============================================\n",
    "    results = []\n",
    "    # 切换model模型为评估模式，关闭dropout等随机因素\n",
    "    model.eval()\n",
    "    for batch in test_data_loader:\n",
    "        input_ids, token_type_ids, qids = batch\n",
    "        # 喂数据给模型\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # 预测分类\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        qids = qids.numpy().tolist()\n",
    "        results.extend(zip(qids, idx))\n",
    "    #============================================\n",
    "    import numpy as np\n",
    "    predict_label = [i[1] for i in results]\n",
    "    test.insert(0,'predict_label',np.array([i[1] for i in results]))\n",
    "    test.to_csv('comment_{}_label.csv'.format(policy_name), index=False,encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy1\n",
      "policy2\n",
      "policy3\n",
      "policy4\n",
      "policy5\n",
      "policy6\n",
      "policy7\n",
      "policy9\n",
      "policy10\n",
      "policy11\n"
     ]
    }
   ],
   "source": [
    "#======================================================================\n",
    "get_label_tocsv('policy1')\n",
    "get_label_tocsv('policy2')\n",
    "get_label_tocsv('policy3')\n",
    "get_label_tocsv('policy4')\n",
    "get_label_tocsv('policy5')\n",
    "get_label_tocsv('policy6')\n",
    "get_label_tocsv('policy7')\n",
    "get_label_tocsv('policy9')\n",
    "get_label_tocsv('policy10')\n",
    "get_label_tocsv('policy11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿predict_label,tweet_id,policy_id,comment_time,pure_blog\r\n",
      "1,0,policy1,2020-01-23 17:03:00,通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市通报湖北省地级市\r\n",
      "2,1,policy1,2020-01-23 17:04:00,孝感孝感孝感\r\n",
      "1,2,policy1,2020-01-23 17:04:00,宜昌宜昌宜昌宜昌宜昌\r\n",
      "1,3,policy1,2020-01-23 17:04:00,荆门！孝感！黄石！\r\n",
      "0,4,policy1,2020-01-23 17:04:00,十堰！这些地方为什么不动态监控加强防护\r\n",
      "1,5,policy1,2020-01-23 17:05:00,孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感孝感\r\n",
      "0,6,policy1,2020-01-23 17:04:00,通报湖北，亲人在十堰，我们真的太需要多一些了解了！！！为什么还不能通报各市详细情况！！！！！！\r\n",
      "0,7,policy1,2020-01-23 17:07:00,孝感孝感！孝汉城铁一天那么多趟！一例都没有通报我打死都不信！除了吃湖北三分之一经济的武汉难道其他地级市不配有名字？\r\n",
      "2,8,policy1,2020-01-23 17:05:00,孝感孝感孝感孝感！！！！\r\n"
     ]
    }
   ],
   "source": [
    "!head comment_policy1_label.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
